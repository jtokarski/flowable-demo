
== Deep Learning


=== Introduction

* Inventors have long dreamed of creating machines that think. This desire dates
  back to at least the time of ancient Greece. The mythical figures Pygmalion,
  Daedalus, and Hephaestus may all be interpreted as legendary inventors, and
  Galatea, Talos, and Pandora may all be regarded as artificial life.

* When programmable computers were first conceived and over a hundred years before
  one was built, people wondered whether such machines might become intelligent.

* Today, artificial intelligence (AI) is a thriving field with many practical
  applications and active research topics.

* We look to intelligent software to automate routine labor, understand speech or images,
  make diagnoses in medicine and support basic scientific research.

* The true challenge to artificial intelligence proved to be solving
  the tasks that are easy for people to perform but hard for people to describe
  formallyâ€”problems that we solve intuitively, that feel automatic, like recognizing
  spoken words or faces in images.

* This solution is to allow computers to learn from experience and understand the world
  in terms of a hierarchy of concepts, with each concept defined in terms
  of its relation to simpler concepts. If we draw a graph showing how these
  concepts are built on top of each other, the graph is deep, with many layers.
  For this reason, we call this approach to AI deep learning.

* One of the key challenges in artificial intelligence is how to get
  informal knowledge into a computer.

* The difficulties faced by systems relying on hard-coded knowledge suggest
  that AI systems need the ability to acquire their own knowledge, by extracting
  patterns from raw data. This capability is known as machine learning.

* The introduction of *machine learning* allowed computers to tackle problems involving
  knowledge of the real world and make decisions that appear subjective. A simple
  machine learning algorithm called *logistic regression* can determine whether to
  recommend cesarean delivery (Mor-Yosef et al., 1990). A simple machine learning
  algorithm called *naive Bayes* can separate legitimate e-mail from spam e-mail.

* *Dependence on representations* is a general phenomenon that appears throughout
  computer science and even daily life. In computer science, operations
  such as searching a collection of data can proceed exponentially faster
  if the collection is structured and indexed intelligently.

* Many artificial intelligence tasks can be solved by *designing the right set of features*
  to extract for that task, then providing these features to a simple machine learning algorithm.
  For example, a useful feature for *speaker identification* from sound is an estimate
  of the *size of speaker's vocal tract*. It therefore gives a strong clue as to whether the speaker
  is a man, woman, or child.

* One solution to this problem is to use machine learning to discover not only
  *the mapping from representation to output* but also the *representation itself*.
  This approach is known as representation learning. Learned representations
  often result in much better performance than can be obtained with hand-designed
  representations.

* The quintessential example of a *representation learning algorithm* is the *autoencoder*.
  An autoencoder is the combination of an *encoder* function that
  converts the input data into a different representation, and a *decoder* function
  that converts the new representation back into the original format.







== TensorFlow Core

=== Keras

* Keras is the high-level API of the TensorFlow platform. It provides an
  approachable, highly-productive interface for solving machine learning (ML)
  problems, with a focus on modern deep learning. Keras covers every step
  of the machine learning workflow, from data processing to hyperparameter tuning
  to deployment. It was developed with a focus on enabling fast experimentation.

* With Keras, you have full access to the scalability and cross-platform
  capabilities of TensorFlow. You can run Keras on a TPU Pod or large clusters
  of GPUs, and you can export Keras models to run in the browser or on mobile
  devices. You can also serve Keras models via a web API.

* The core data structures of Keras are layers and models. A layer is a simple
  input/output transformation, and a model is a directed acyclic graph (DAG)
  of layers.

* I would like to learn *neural networks* and *deep learning*. But it seems to be
  an overwhelming subject with steep learning curve. I need to divide it somehow.
  In order to get familiar with Tensorflow itself I thought of using it for
  completing simple tasks. Tasks for which Tensorflow obviously will be an overkill,
  but will just help me familiarize with APIs. Do you think it's possible approach?
  If so can you give some examples of simple tasks I can do with Tensorflow?

* In the context of machine learning, could you explain the term *factors of variation*?





== Attribution

This documents contains quotes from:

* book *Deep Learning* by *Ian Goodfellow*, *Yoshua Bengio*, *Aaron Courville*,
  Published by *MIT Press*.
  See: https://www.deeplearningbook.org/[deeplearningbook.org]

* TensorFlow documentation. See: https://www.tensorflow.org/guide[TensorFlow Core]




